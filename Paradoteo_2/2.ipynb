{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Παραδοτέο 2\n",
        "\n",
        "---\n",
        "ΕΠΕΞΕΡΓΑΣΙΑ ΦΥΣΙΚΗΣ ΓΛΩΣΣΑΣ <br>\n",
        "Απαλλακτική Εργασία Σεπτεμβρίου 2025 <br>\n",
        "Γιαννακόπουλος Νικόλαος Ιωάννης | Π22029\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "50AAxiOZYsGo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Απαραίτητες βιβλιοθήκες"
      ],
      "metadata": {
        "id": "RGTtIpRWf_bG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFuY9f5SQdBm"
      },
      "outputs": [],
      "source": [
        "pip install numpy pandas matplotlib seaborn scikit-learn nltk gensim sentence-transformers transformers torch tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ο κώδικας"
      ],
      "metadata": {
        "id": "KhxgCbAZgDzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import gensim.downloader as api\n",
        "from gensim.models import Word2Vec, FastText\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Necessary setup\n",
        "packages = ['sentence-transformers', 'gensim', 'seaborn', 'nltk']\n",
        "\n",
        "for pkg in packages:\n",
        "    try:\n",
        "        __import__(pkg.replace('-', '_'))\n",
        "    except ImportError:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
        "\n",
        "# Download NLTK data - Updated to include punkt_tab\n",
        "nltk_data = ['punkt_tab', 'stopwords', 'wordnet', 'averaged_perceptron_tagger', 'omw-1.4']\n",
        "for resource in nltk_data:\n",
        "    try:\n",
        "        nltk.download(resource, quiet=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not download {resource}: {e}\")\n",
        "\n",
        "# Also download the legacy punkt for compatibility\n",
        "try:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Texts\n",
        "texts = {\n",
        "    \"original\": {\n",
        "        \"text1\": \"Thank your message to show our words to the doctor, as his next contract checking, to all of us. I got this message to see the approved message. In fact, I have received the message from the professor, to show me, this, a couple of days ago. I am very appreciated the full support of the professor, for our Springer proceedings publication\",\n",
        "        \"text2\": \"We should be grateful, I mean all of us, for the acceptance and efforts until the Springer link came finally last week, I think. Also, kindly remind me please, if the doctor still plan for the acknowledgments section edit before he sending again.\"\n",
        "    },\n",
        "    \"reconstructed\": {\n",
        "        \"naive\": {\n",
        "            \"text1\": \"Thank you for your message to show our words to the doctor, during his next contract review, to all of us.\",\n",
        "            \"text2\": \"We should be grateful, i mean all of us, for the acceptance and efforts until the springer link came finally last week, i think.\"\n",
        "        },\n",
        "        \"t5\": {\n",
        "            \"text1\": \"Thank you for forwarding our message to the doctor regarding his upcoming contract review. I greatly appreciate the professor's full support for our Springer proceedings publication.\",\n",
        "            \"text2\": \"We should all be grateful for the acceptance and efforts leading to the Springer link's arrival last week. Please remind me if the doctor plans to edit the acknowledgments before resubmission.\"\n",
        "        },\n",
        "        \"bert\": {\n",
        "            \"text1\": \"Thank you for your message showing our words to the doctor for his next contract review. I received the professor's message a couple of days ago and very much appreciate the full support for our Springer proceedings publication.\",\n",
        "            \"text2\": \"We should be grateful, I mean all of us, for the acceptance and efforts until the Springer link finally came last week. Also, please kindly remind me if the doctor still plans to edit the acknowledgments section before sending again.\"\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Custom NLP Preprocessing Pipeline\n",
        "class TextProcessor:\n",
        "\n",
        "    # Constructor\n",
        "    def __init__(self):\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        try:\n",
        "            self.stop_words = set(stopwords.words('english'))\n",
        "        except:\n",
        "            self.stop_words = set()\n",
        "\n",
        "    # Comprehensive Text Preprocessing\n",
        "    def clean_text(self, text, keep_stopwords=False):\n",
        "        try:\n",
        "            tokens = word_tokenize(text.lower())\n",
        "        except LookupError:\n",
        "            # Fallback tokenization if NLTK fails\n",
        "            tokens = text.lower().split()\n",
        "\n",
        "        if keep_stopwords:\n",
        "            cleaned = [self.lemmatizer.lemmatize(t) for t in tokens if t.isalnum()]\n",
        "        else:\n",
        "            cleaned = [self.lemmatizer.lemmatize(t) for t in tokens\n",
        "                      if t.isalnum() and t not in self.stop_words]\n",
        "\n",
        "        return cleaned\n",
        "\n",
        "    # Extract Vocabulary from Collection of Texts\n",
        "    def get_vocab(self, text_list):\n",
        "        vocab = set()\n",
        "        for txt in text_list:\n",
        "            vocab.update(self.clean_text(txt))\n",
        "\n",
        "        return sorted(list(vocab))\n",
        "\n",
        "    # Analyze Sentence Structure\n",
        "    def analyze_structure(self, text):\n",
        "        try:\n",
        "            sentences = sent_tokenize(text)\n",
        "        except LookupError:\n",
        "            # Fallback sentence splitting if NLTK fails\n",
        "            sentences = text.split('.')\n",
        "            sentences = [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "        return {\n",
        "            'sentence_count': len(sentences),\n",
        "            'avg_length': np.mean([len(self.clean_text(s, keep_stopwords=True)) for s in sentences]) if sentences else 0,\n",
        "            'sentences': sentences\n",
        "        }\n",
        "\n",
        "# Comprehensive Embeddings Analyzer with Multiple Models\n",
        "class EmbeddingAnalyzer:\n",
        "\n",
        "    # Constructor\n",
        "    def __init__(self):\n",
        "        self.models = {}\n",
        "        self.processor = TextProcessor()\n",
        "        self._load_models()\n",
        "\n",
        "    # Load Embedding Models\n",
        "    def _load_models(self):\n",
        "        # Load sentence transformer - most reliable for our use case\n",
        "        try:\n",
        "            self.models['sbert'] = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "            print(\"Loaded sentence transformer\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to load sentence transformer: {e}\")\n",
        "\n",
        "        # Try to load GloVe - useful for word-level analysis\n",
        "        try:\n",
        "            self.models['glove'] = api.load('glove-twitter-25')\n",
        "            print(\"Loaded GloVe embeddings\")\n",
        "        except:\n",
        "            print(\"GloVe embeddings not available\")\n",
        "\n",
        "        self._train_custom_w2v()\n",
        "\n",
        "    # Create Word2Vec & FastText Embedding\n",
        "    def _train_custom_w2v(self):\n",
        "        # Collect all text for training\n",
        "        all_text = []\n",
        "        all_text.extend(texts[\"original\"].values())\n",
        "        for method in texts[\"reconstructed\"].values():\n",
        "            all_text.extend(method.values())\n",
        "\n",
        "        # Tokenize for training\n",
        "        corpus = [self.processor.clean_text(txt, keep_stopwords=True) for txt in all_text]\n",
        "        corpus = [tokens for tokens in corpus if len(tokens) > 0]\n",
        "\n",
        "        if corpus:\n",
        "            try:\n",
        "                self.models['w2v'] = Word2Vec(corpus, vector_size=100, window=5,\n",
        "                                           min_count=1, sg=1, epochs=10)\n",
        "                print(\"Trained Word2Vec model\")\n",
        "            except Exception as e:\n",
        "                print(f\"W2V training failed: {e}\")\n",
        "\n",
        "            try:\n",
        "                self.models['fasttext'] = FastText(corpus, vector_size=100, window=5,\n",
        "                                                 min_count=1, sg=1, epochs=10)\n",
        "                print(\"Trained FastText model\")\n",
        "            except Exception as e:\n",
        "                print(f\"FastText training failed: {e}\")\n",
        "\n",
        "    # Get embedding for a single word\n",
        "    def get_word_vector(self, word, model='glove'):\n",
        "        word = word.lower()\n",
        "        if model == 'glove' and 'glove' in self.models:\n",
        "            try:\n",
        "                return self.models['glove'][word]\n",
        "            except KeyError:\n",
        "                return None\n",
        "        elif model == 'w2v' and 'w2v' in self.models:\n",
        "            try:\n",
        "                return self.models['w2v'].wv[word]\n",
        "            except KeyError:\n",
        "                return None\n",
        "        elif model == 'fasttext' and 'fasttext' in self.models:\n",
        "            try:\n",
        "                return self.models['fasttext'].wv[word]\n",
        "            except KeyError:\n",
        "                return None\n",
        "\n",
        "        return None\n",
        "\n",
        "    # Get Embedding For Entire Text\n",
        "    def get_text_vector(self, text, method='sbert'):\n",
        "        if method == 'sbert' and 'sbert' in self.models:\n",
        "            return self.models['sbert'].encode(text)\n",
        "        elif method in ['glove', 'w2v', 'fasttext']:\n",
        "            tokens = self.processor.clean_text(text)\n",
        "            vectors = []\n",
        "            for token in tokens:\n",
        "                vec = self.get_word_vector(token, method)\n",
        "                if vec is not None:\n",
        "                    vectors.append(vec)\n",
        "\n",
        "            return np.mean(vectors, axis=0) if vectors else None\n",
        "\n",
        "        return None\n",
        "\n",
        "    # Compute cosine similarities\n",
        "    def calculate_similarities(self):\n",
        "        results = {}\n",
        "        available_methods = [m for m in ['sbert', 'glove', 'w2v', 'fasttext'] if m in self.models]\n",
        "        for method in available_methods:\n",
        "            results[method] = {}\n",
        "            for text_id in texts[\"original\"].keys():\n",
        "                orig_text = texts[\"original\"][text_id]\n",
        "                orig_vec = self.get_text_vector(orig_text, method)\n",
        "                if orig_vec is not None:\n",
        "                    sims = {}\n",
        "                    for recon_method, recon_texts in texts[\"reconstructed\"].items():\n",
        "                        recon_text = recon_texts[text_id]\n",
        "                        recon_vec = self.get_text_vector(recon_text, method)\n",
        "                        if recon_vec is not None:\n",
        "                            sim = cosine_similarity(orig_vec.reshape(1, -1),\n",
        "                                                  recon_vec.reshape(1, -1))[0, 0]\n",
        "                            sims[recon_method] = sim\n",
        "                    results[method][text_id] = sims\n",
        "\n",
        "        return results\n",
        "\n",
        "# Visualize text embeddings\n",
        "def plot_embeddings_2d(analyzer, embedding_method='sbert'):\n",
        "    if embedding_method not in analyzer.models:\n",
        "        print(f\"Method {embedding_method} not available\")\n",
        "        return\n",
        "\n",
        "    # Collect all text embeddings\n",
        "    embeddings = []\n",
        "    labels = []\n",
        "\n",
        "    # Original texts\n",
        "    for text_id, text in texts[\"original\"].items():\n",
        "        vec = analyzer.get_text_vector(text, embedding_method)\n",
        "        if vec is not None:\n",
        "            embeddings.append(vec)\n",
        "            labels.append(f\"Original_{text_id}\")\n",
        "\n",
        "    # Reconstructed texts\n",
        "    for method, method_texts in texts[\"reconstructed\"].items():\n",
        "        for text_id, text in method_texts.items():\n",
        "            vec = analyzer.get_text_vector(text, embedding_method)\n",
        "            if vec is not None:\n",
        "                embeddings.append(vec)\n",
        "                labels.append(f\"{method}_{text_id}\")\n",
        "\n",
        "    if len(embeddings) < 2:\n",
        "        print(f\"Not enough embeddings for visualization ({len(embeddings)} found)\")\n",
        "        return\n",
        "\n",
        "    embeddings = np.array(embeddings)\n",
        "\n",
        "    # Create subplots\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "    # PCA visualization\n",
        "    pca = PCA(n_components=2)\n",
        "    embeddings_pca = pca.fit_transform(embeddings)\n",
        "\n",
        "    colors = plt.cm.tab10(np.linspace(0, 1, len(labels)))\n",
        "\n",
        "    for i, (label, color) in enumerate(zip(labels, colors)):\n",
        "        ax1.scatter(embeddings_pca[i, 0], embeddings_pca[i, 1],\n",
        "                   c=[color], s=100, label=label, alpha=0.7)\n",
        "        ax1.annotate(label, (embeddings_pca[i, 0], embeddings_pca[i, 1]),\n",
        "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
        "\n",
        "    ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
        "    ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
        "    ax1.set_title(f'PCA Visualization - {embedding_method.upper()}')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # t-SNE visualization\n",
        "    if len(embeddings) >= 4:\n",
        "        perplexity = min(5, len(embeddings) - 1)\n",
        "        tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)\n",
        "        embeddings_tsne = tsne.fit_transform(embeddings)\n",
        "\n",
        "        for i, (label, color) in enumerate(zip(labels, colors)):\n",
        "            ax2.scatter(embeddings_tsne[i, 0], embeddings_tsne[i, 1],\n",
        "                       c=[color], s=100, label=label, alpha=0.7)\n",
        "            ax2.annotate(label, (embeddings_tsne[i, 0], embeddings_tsne[i, 1]),\n",
        "                        xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
        "\n",
        "        ax2.set_xlabel('t-SNE Component 1')\n",
        "        ax2.set_ylabel('t-SNE Component 2')\n",
        "        ax2.set_title(f't-SNE Visualization - {embedding_method.upper()}')\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "    else:\n",
        "        ax2.text(0.5, 0.5, 'Not enough points\\nfor t-SNE',\n",
        "                ha='center', va='center', transform=ax2.transAxes, fontsize=12)\n",
        "        ax2.set_title('t-SNE (Insufficient Data)')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def visualize_word_embeddings(analyzer, words=None, embedding_method='w2v'):\n",
        "    if embedding_method not in analyzer.models:\n",
        "        print(f\"Method {embedding_method} not available\")\n",
        "        return\n",
        "\n",
        "    if words is None:\n",
        "        # Get common words from our texts\n",
        "        all_text = []\n",
        "        all_text.extend(texts[\"original\"].values())\n",
        "        for method_texts in texts[\"reconstructed\"].values():\n",
        "            all_text.extend(method_texts.values())\n",
        "\n",
        "        vocab = analyzer.processor.get_vocab(all_text)\n",
        "        words = vocab[:15]  # Top 15 words\n",
        "\n",
        "    # Collect word embeddings\n",
        "    embeddings = []\n",
        "    valid_words = []\n",
        "\n",
        "    for word in words:\n",
        "        vec = analyzer.get_word_vector(word, embedding_method)\n",
        "        if vec is not None:\n",
        "            embeddings.append(vec)\n",
        "            valid_words.append(word)\n",
        "\n",
        "    if len(embeddings) < 2:\n",
        "        print(f\"Not enough word embeddings for visualization ({len(embeddings)} found)\")\n",
        "        return\n",
        "\n",
        "    embeddings = np.array(embeddings)\n",
        "\n",
        "    # Create subplots\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "    # PCA visualization\n",
        "    pca = PCA(n_components=2)\n",
        "    embeddings_pca = pca.fit_transform(embeddings)\n",
        "\n",
        "    colors = plt.cm.viridis(np.linspace(0, 1, len(valid_words)))\n",
        "\n",
        "    for i, (word, color) in enumerate(zip(valid_words, colors)):\n",
        "        ax1.scatter(embeddings_pca[i, 0], embeddings_pca[i, 1],\n",
        "                   c=[color], s=100, alpha=0.7)\n",
        "        ax1.annotate(word, (embeddings_pca[i, 0], embeddings_pca[i, 1]),\n",
        "                    xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
        "\n",
        "    ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
        "    ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
        "    ax1.set_title(f'Word Embeddings PCA - {embedding_method.upper()}')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # t-SNE visualization\n",
        "    if len(embeddings) >= 4:\n",
        "        perplexity = min(5, len(embeddings) - 1)\n",
        "        tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)\n",
        "        embeddings_tsne = tsne.fit_transform(embeddings)\n",
        "\n",
        "        for i, (word, color) in enumerate(zip(valid_words, colors)):\n",
        "            ax2.scatter(embeddings_tsne[i, 0], embeddings_tsne[i, 1],\n",
        "                       c=[color], s=100, alpha=0.7)\n",
        "            ax2.annotate(word, (embeddings_tsne[i, 0], embeddings_tsne[i, 1]),\n",
        "                        xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
        "\n",
        "        ax2.set_xlabel('t-SNE Component 1')\n",
        "        ax2.set_ylabel('t-SNE Component 2')\n",
        "        ax2.set_title(f'Word Embeddings t-SNE - {embedding_method.upper()}')\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "    else:\n",
        "        ax2.text(0.5, 0.5, 'Not enough points\\nfor t-SNE',\n",
        "                ha='center', va='center', transform=ax2.transAxes, fontsize=12)\n",
        "        ax2.set_title('t-SNE (Insufficient Data)')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_results(similarity_data):\n",
        "    # Convert to DataFrame for easier plotting\n",
        "    plot_data = []\n",
        "    for method, texts_data in similarity_data.items():\n",
        "        for text_id, sims in texts_data.items():\n",
        "            for recon_method, sim_score in sims.items():\n",
        "                plot_data.append({\n",
        "                    'embedding': method,\n",
        "                    'text': text_id,\n",
        "                    'reconstruction': recon_method,\n",
        "                    'similarity': sim_score\n",
        "                })\n",
        "\n",
        "    if not plot_data:\n",
        "        print(\"No similarity data to plot\")\n",
        "        return\n",
        "\n",
        "    df = pd.DataFrame(plot_data)\n",
        "\n",
        "    # Create heatmap\n",
        "    pivot = df.pivot_table(values='similarity',\n",
        "                          index=['embedding', 'text'],\n",
        "                          columns='reconstruction')\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.heatmap(pivot, annot=True, cmap='viridis', fmt='.3f')\n",
        "    plt.title('Text Reconstruction Quality by Embedding Method')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return df\n",
        "\n",
        "def tfidf_comparison():\n",
        "    # TF-IDF Based Similarity Analysis\n",
        "    all_texts = []\n",
        "    labels = []\n",
        "\n",
        "    # Add original texts\n",
        "    for text_id, text in texts[\"original\"].items():\n",
        "        all_texts.append(text)\n",
        "        labels.append(f\"original_{text_id}\")\n",
        "\n",
        "    # Add reconstructed texts\n",
        "    for method, method_texts in texts[\"reconstructed\"].items():\n",
        "        for text_id, text in method_texts.items():\n",
        "            all_texts.append(text)\n",
        "            labels.append(f\"{method}_{text_id}\")\n",
        "\n",
        "    vectorizer = TfidfVectorizer(stop_words='english', max_features=100)\n",
        "    tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
        "    similarities = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.heatmap(similarities, annot=True, xticklabels=labels, yticklabels=labels,\n",
        "                cmap='coolwarm', fmt='.2f')\n",
        "    plt.title('TF-IDF Similarity Matrix')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return similarities\n",
        "\n",
        "def analyze_structure():\n",
        "    processor = TextProcessor()\n",
        "\n",
        "    print(\"Text Structure Analysis:\")\n",
        "\n",
        "    for text_id in texts[\"original\"].keys():\n",
        "        orig = texts[\"original\"][text_id]\n",
        "        orig_stats = processor.analyze_structure(orig)\n",
        "\n",
        "        print(f\"\\n{text_id}:\")\n",
        "        print(f\"Original: {orig_stats['sentence_count']} sentences, {orig_stats['avg_length']:.1f} avg words\")\n",
        "\n",
        "        for method, method_texts in texts[\"reconstructed\"].items():\n",
        "            recon_stats = processor.analyze_structure(method_texts[text_id])\n",
        "            print(f\"{method:8}: {recon_stats['sentence_count']} sentences, {recon_stats['avg_length']:.1f} avg words\")\n",
        "\n",
        "# Main Comprehensive Analysis Function\n",
        "def run_analysis():\n",
        "    # Structure analysis\n",
        "    analyze_structure()\n",
        "\n",
        "    # Embedding similarity analysis\n",
        "    analyzer = EmbeddingAnalyzer()\n",
        "    similarities = analyzer.calculate_similarities()\n",
        "\n",
        "    print(\"\\nSimilarity Results:\")\n",
        "\n",
        "    for method, data in similarities.items():\n",
        "        print(f\"\\n{method.upper()} embeddings:\")\n",
        "        for text_id, sims in data.items():\n",
        "            print(f\"  {text_id}:\")\n",
        "            for recon_method, score in sims.items():\n",
        "                print(f\"    {recon_method:8}: {score:.4f}\")\n",
        "\n",
        "    # Create visualizations\n",
        "    df = plot_results(similarities)\n",
        "    tfidf_sim = tfidf_comparison()\n",
        "\n",
        "    # Text embeddings visualization\n",
        "    available_methods = [m for m in ['sbert', 'glove', 'w2v', 'fasttext'] if m in analyzer.models]\n",
        "    for method in available_methods:\n",
        "        print(f\"Visualizing {method} text embeddings\")\n",
        "        plot_embeddings_2d(analyzer, method)\n",
        "\n",
        "    # Word embeddings visualization\n",
        "    word_methods = [m for m in ['w2v', 'fasttext', 'glove'] if m in analyzer.models]\n",
        "    for method in word_methods:\n",
        "        print(f\"Visualizing {method} word embeddings\")\n",
        "        visualize_word_embeddings(analyzer, embedding_method=method)\n",
        "\n",
        "    # Find best method\n",
        "    if similarities and 'sbert' in similarities:\n",
        "        method_scores = {}\n",
        "        sbert_data = similarities['sbert']\n",
        "\n",
        "        for recon_method in texts[\"reconstructed\"].keys():\n",
        "            scores = []\n",
        "            for text_data in sbert_data.values():\n",
        "                if recon_method in text_data:\n",
        "                    scores.append(text_data[recon_method])\n",
        "\n",
        "            if scores:\n",
        "                method_scores[recon_method] = np.mean(scores)\n",
        "\n",
        "        if method_scores:\n",
        "            best = max(method_scores, key=method_scores.get)\n",
        "            print(f\"\\nBest reconstruction method: {best} (avg similarity: {method_scores[best]:.3f})\")\n",
        "\n",
        "    return similarities, analyzer\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results = run_analysis()"
      ],
      "metadata": {
        "id": "HmsR6mMVT7a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6a3d040-1c31-48ee-997b-f65b7279fe1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Structure Analysis:\n",
            "\n",
            "text1:\n",
            "Original: 4 sentences, 15.5 avg words\n",
            "naive   : 1 sentences, 21.0 avg words\n",
            "t5      : 2 sentences, 13.0 avg words\n",
            "bert    : 2 sentences, 19.0 avg words\n",
            "\n",
            "text2:\n",
            "Original: 2 sentences, 21.5 avg words\n",
            "naive   : 1 sentences, 24.0 avg words\n",
            "t5      : 2 sentences, 15.5 avg words\n",
            "bert    : 2 sentences, 20.0 avg words\n",
            "Loaded sentence transformer\n"
          ]
        }
      ]
    }
  ]
}